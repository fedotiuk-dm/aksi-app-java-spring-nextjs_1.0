#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏—Ö Excel –¥–∞–Ω–∏—Ö –≤—ñ–¥ —Å–º—ñ—Ç—Ç—è
–í–∏–¥–∞–ª—è—î –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏, –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ñ —Å–∏–º–≤–æ–ª–∏, –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
"""

import pandas as pd
import json
import os
import shutil
from pathlib import Path

def clean_excel_data():
    """–û—á–∏—â–µ–Ω–Ω—è —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö –≤—ñ–¥ —Å–º—ñ—Ç—Ç—è"""

    input_dir = "parsed_data"
    output_dir = "cleaned_data"

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –≤—Ö—ñ–¥–Ω–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    if not os.path.exists(input_dir):
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {input_dir} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
        return

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (–∑ –≤–∏–¥–∞–ª–µ–Ω–Ω—è–º —ñ—Å–Ω—É—é—á–æ—ó)
    if os.path.exists(output_dir):
        print(f"üóëÔ∏è  –í–∏–¥–∞–ª–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó: {output_dir}")
        shutil.rmtree(output_dir)

    os.makedirs(output_dir)
    print(f"üìÅ –°—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é: {output_dir}")

    # –ß–∏—Ç–∞–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    index_file = os.path.join(input_dir, "index.json")
    if not os.path.exists(index_file):
        print("‚ùå –Ü–Ω–¥–µ–∫—Å–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        return

    with open(index_file, 'r', encoding='utf-8') as f:
        index_data = json.load(f)

    print(f"\nüìä –û—á–∏—â–µ–Ω–Ω—è {index_data['total_sheets']} –∞—Ä–∫—É—à—ñ–≤...")

    # –ê—Ä–∫—É—à—ñ —â–æ –º—ñ—Å—Ç—è—Ç—å —Ç—ñ–ª—å–∫–∏ —Å–º—ñ—Ç—Ç—è (–ø–æ–≤–Ω—ñ—Å—Ç—é –≤–∏–¥–∞–ª—è—î–º–æ)
    junk_sheets = [
        "-Template-",
        "Sheet45",
        "Sheet46"
    ]

    # –ö–æ–ª–æ–Ω–∫–∏ —â–æ –∑–∞–≤–∂–¥–∏ —î —Å–º—ñ—Ç—Ç—è–º
    junk_columns = [
        "Unnamed: 0", "xxx", "---", "---.1", "---.2",
        "a", "a.1", "a.2", "a.3", "a.4",
        "Unnamed: 11", "Unnamed: 12", "Unnamed: 17"
    ]

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—á–∏—â–µ–Ω–Ω—è
    cleaning_stats = {
        "total_sheets": 0,
        "junk_sheets_removed": 0,
        "cleaned_sheets": 0,
        "total_rows_before": 0,
        "total_rows_after": 0,
        "removed_rows": 0
    }

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó datetime –æ–±'—î–∫—Ç—ñ–≤
    def convert_datetime(obj):
        if isinstance(obj, pd.Timestamp):
            return obj.strftime('%Y-%m-%d %H:%M:%S')
        elif pd.isna(obj):
            return None
        else:
            return obj

    cleaned_sheets_data = {}

    for sheet_name, sheet_info in index_data["sheets"].items():
        cleaning_stats["total_sheets"] += 1
        cleaning_stats["total_rows_before"] += sheet_info["rows"]

        print(f"\nüîÑ –û–±—Ä–æ–±–∫–∞ –∞—Ä–∫—É—à–∞: {sheet_name}")

        # –ü–æ–≤–Ω—ñ—Å—Ç—é –≤–∏–¥–∞–ª—è—î–º–æ –∞—Ä–∫—É—à—ñ —â–æ –º—ñ—Å—Ç—è—Ç—å —Ç—ñ–ª—å–∫–∏ —Å–º—ñ—Ç—Ç—è
        if sheet_name in junk_sheets:
            print(f"   üóëÔ∏è  –ê—Ä–∫—É—à {sheet_name} –º—ñ—Å—Ç–∏—Ç—å —Ç—ñ–ª—å–∫–∏ —Å–º—ñ—Ç—Ç—è - –≤–∏–¥–∞–ª—è—î–º–æ –ø–æ–≤–Ω—ñ—Å—Ç—é")
            cleaning_stats["junk_sheets_removed"] += 1
            continue

        # –®–ª—è—Ö –¥–æ CSV —Ñ–∞–π–ª—É (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ CSV –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è)
        csv_file = os.path.join(input_dir, f"{sheet_name}.csv")

        if not os.path.exists(csv_file):
            print(f"   ‚ö†Ô∏è  –§–∞–π–ª {csv_file} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
            continue

        try:
            # –ß–∏—Ç–∞–Ω–Ω—è CSV —Ñ–∞–π–ª—É
            df = pd.read_csv(csv_file)

            # –í–∏–¥–∞–ª—è—î–º–æ —Å–º—ñ—Ç—Ç—î–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            columns_to_drop = [col for col in junk_columns if col in df.columns]
            if columns_to_drop:
                df = df.drop(columns=columns_to_drop)
                print(f"   üßπ –í–∏–¥–∞–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {columns_to_drop}")

            # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏
            df_cleaned = df.dropna(how='all')

            # –í–∏–¥–∞–ª—è—î–º–æ —Ä—è–¥–∫–∏ —â–æ –º—ñ—Å—Ç—è—Ç—å —Ç—ñ–ª—å–∫–∏ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ñ —Å–∏–º–≤–æ–ª–∏ –∞–±–æ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
            def is_junk_row(row):
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ä—è–¥–æ–∫ –º—ñ—Å—Ç–∏—Ç—å —Ç—ñ–ª—å–∫–∏ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ñ —Å–∏–º–≤–æ–ª–∏
                text_content = ' '.join(str(val) for val in row.values if pd.notna(val))
                if any(char in text_content for char in ['‚ñ¨', '‚ñà', '‚ñì', '‚ñí', '‚ñë']):
                    return True
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
                if any(keyword in text_content.lower() for keyword in [
                    'copy & paste', 'example', 'like the example', 'most popular',
                    'platform', 'service requested', 'account login', 'account password'
                ]):
                    return True
                return False

            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ä—è–¥–∫–∏
            rows_before = len(df_cleaned)
            df_cleaned = df_cleaned[~df_cleaned.apply(is_junk_row, axis=1)]
            rows_after = len(df_cleaned)
            removed_rows = rows_before - rows_after

            if removed_rows > 0:
                print(f"   üßπ –í–∏–¥–∞–ª–µ–Ω–æ {removed_rows} —Å–º—ñ—Ç—Ç—î–≤–∏—Ö —Ä—è–¥–∫—ñ–≤")

            # –í–∏–¥–∞–ª—è—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ —â–æ —Å—Ç–∞–ª–∏ –ø–æ–≤–Ω—ñ—Å—Ç—é –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ –ø—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è
            df_cleaned = df_cleaned.dropna(axis=1, how='all')

            # –ó–∞–ø–æ–≤–Ω—é—î–º–æ NaN –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ —Ä—è–¥–∫–∞–º–∏
            df_cleaned = df_cleaned.fillna('')

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—á–∏—â–µ–Ω—ñ –¥–∞–Ω—ñ
            if not df_cleaned.empty:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—ñ —Ñ–∞–π–ª–∏
                json_file = os.path.join(output_dir, f"{sheet_name}.json")
                csv_clean_file = os.path.join(output_dir, f"{sheet_name}.csv")
                structured_json_file = os.path.join(output_dir, f"{sheet_name}_structured.json")

                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º
                def filter_data_row(row):
                    # –í–∏–¥–∞–ª—è—î–º–æ —Ä—è–¥–∫–∏ –∑ –ª—ñ–∫–∞–º–∏ –Ω–∞ –∑–æ–≤–Ω—ñ—à–Ω—ñ —Å–∞–π—Ç–∏ –∞–±–æ —Å–º—ñ—Ç—Ç—è–º
                    for value in row.values():
                        if isinstance(value, str):
                            # –õ—ñ–Ω–∫–∏ –Ω–∞ –±—É—Å—Ç–∏–Ω–≥ —Å–∞–π—Ç–∏ —è–∫—ñ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ
                            if any(site in value.lower() for site in [
                                'playerauctions.com', 'epicnpc.com', 'ownedcore.com',
                                'elitepvpers.com', 'sythe.org', 'gamerebels.com',
                                'mmobc.com', 'raiditem.com', 'igvault.com'
                            ]):
                                return False
                            # –í–∏–¥–∞–ª—è—î–º–æ —Ä—è–¥–∫–∏ –∑ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
                            if any(char in value for char in ['‚ñ¨', '‚ñà', '‚ñì', '‚ñí', '‚ñë', '‚ñ†', '‚ñ°']):
                                return False
                            # –í–∏–¥–∞–ª—è—î–º–æ —Ä—è–¥–∫–∏ –∑ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º–∏ —Ç–∞ —Å–º—ñ—Ç—Ç—è–º
                            if any(garbage in value.lower() for garbage in [
                                'copy & paste like the example', 'most popular',
                                'platform (pc/ps4/xbox):', 'service requested:',
                                'account login:', 'account password:', 'email for updates:'
                            ]):
                                return False
                    return True

                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∞–Ω—ñ
                data_dict = df_cleaned.to_dict('records')
                filtered_data = [row for row in data_dict if filter_data_row(row)]

                # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –ø–æ—Ä–æ–∂–Ω—ñ –æ–±'—î–∫—Ç–∏ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä)
                def is_meaningful_row(row):
                    # –î–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ —Ñ–∞–π–ª—ñ–≤ —Ä—ñ–∑–Ω—ñ –≤–∞–∂–ª–∏–≤—ñ –ø–æ–ª—è
                    if 'Discord' in row:  # A-Boosters —Ñ–∞–π–ª
                        important_fields = ['Discord', 'Contact Email', 'Game 1']
                    elif 'Site$' in row:  # –Ü–≥—Ä–æ–≤—ñ —Ñ–∞–π–ª–∏
                        important_fields = ['Site$', 'PA$', 'Game Name']
                    else:  # –Ü–Ω—à—ñ —Ñ–∞–π–ª–∏
                        important_fields = list(row.keys())[:3]  # –ü–µ—Ä—à—ñ 3 –ø–æ–ª—è

                    has_meaningful_data = False

                    for field in important_fields:
                        if field in row:
                            value = row[field]
                            if isinstance(value, str) and value.strip() and value.strip() not in ['', '0', '0.0']:
                                has_meaningful_data = True
                                break
                            elif pd.notna(value) and str(value).strip() and str(value).strip() not in ['0', '0.0', 'nan']:
                                has_meaningful_data = True
                                break

                    return has_meaningful_data

                filtered_data = [row for row in filtered_data if is_meaningful_row(row)]

                # –Ø–∫—â–æ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –Ω–µ–º–∞—î –¥–∞–Ω–∏—Ö, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ñ–∞–π–ª
                if not filtered_data:
                    print(f"   ‚ö†Ô∏è  –ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∞—Ä–∫—É—à {sheet_name} —Å—Ç–∞–≤ –ø–æ—Ä–æ–∂–Ω—ñ–º")
                    return

                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ JSON
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(filtered_data, f, indent=2, ensure_ascii=False, default=str)

                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ CSV –∑ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
                filtered_df = pd.DataFrame(filtered_data)
                filtered_df.to_csv(csv_clean_file, index=False, encoding='utf-8-sig')

                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π JSON
                structured_data_dict = filtered_data.copy()
                for record in structured_data_dict:
                    for key, value in record.items():
                        record[key] = convert_datetime(value)

                structured_data = {
                    "sheet_name": sheet_name,
                    "total_rows": len(filtered_data),
                    "total_columns": len(df_cleaned.columns),
                    "columns": list(df_cleaned.columns),
                    "data": structured_data_dict
                }
                with open(structured_json_file, 'w', encoding='utf-8') as f:
                    json.dump(structured_data, f, indent=2, ensure_ascii=False, default=str)

                print(f"   ‚úÖ –û—á–∏—â–µ–Ω–∏–π —Ñ–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {len(filtered_data)} —Ä—è–¥–∫—ñ–≤, {len(df_cleaned.columns)} –∫–æ–ª–æ–Ω–æ–∫")

                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—á–∏—â–µ–Ω–∏–π –∞—Ä–∫—É—à
                cleaned_sheets_data[sheet_name] = {
                    "original_rows": sheet_info["rows"],
                    "cleaned_rows": len(filtered_data),
                    "removed_rows": sheet_info["rows"] - len(filtered_data),
                    "original_columns": sheet_info["columns"],
                    "cleaned_columns": len(df_cleaned.columns),
                    "removed_columns": sheet_info["columns"] - len(df_cleaned.columns),
                    "files": {
                        "json": f"{sheet_name}.json",
                        "csv": f"{sheet_name}.csv",
                        "structured_json": f"{sheet_name}_structured.json"
                    }
                }

                cleaning_stats["total_rows_after"] += len(filtered_data)
                cleaning_stats["cleaned_sheets"] += 1
            else:
                print(f"   ‚ö†Ô∏è  –ü—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è –∞—Ä–∫—É—à {sheet_name} —Å—Ç–∞–≤ –ø–æ—Ä–æ–∂–Ω—ñ–º")

        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ {sheet_name}: {str(e)}")
            continue

    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —ñ–Ω–¥–µ–∫—Å–Ω–∏–π —Ñ–∞–π–ª
    cleaned_index_data = {
        "source_file": index_data["source_file"],
        "total_original_sheets": index_data["total_sheets"],
        "total_cleaned_sheets": cleaning_stats["cleaned_sheets"],
        "junk_sheets_removed": cleaning_stats["junk_sheets_removed"],
        "cleaning_stats": {
            "total_rows_before": cleaning_stats["total_rows_before"],
            "total_rows_after": cleaning_stats["total_rows_after"],
            "removed_rows": cleaning_stats["total_rows_before"] - cleaning_stats["total_rows_after"],
            "data_reduction_percent": round(
                (1 - cleaning_stats["total_rows_after"] / max(cleaning_stats["total_rows_before"], 1)) * 100, 2
            )
        },
        "sheets": cleaned_sheets_data
    }

    cleaned_index_file = os.path.join(output_dir, "index.json")
    with open(cleaned_index_file, 'w', encoding='utf-8') as f:
        json.dump(cleaned_index_data, f, indent=2, ensure_ascii=False)

    print("\nüéâ –û—á–∏—â–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó: {output_dir}")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è:")
    print(f"   - –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∞—Ä–∫—É—à—ñ–≤: {cleaning_stats['total_sheets']}")
    print(f"   - –í–∏–¥–∞–ª–µ–Ω–æ —Å–º—ñ—Ç—Ç—î–≤–∏—Ö –∞—Ä–∫—É—à—ñ–≤: {cleaning_stats['junk_sheets_removed']}")
    print(f"   - –û—á–∏—â–µ–Ω–æ –∞—Ä–∫—É—à—ñ–≤: {cleaning_stats['cleaned_sheets']}")
    print(f"   - –†—è–¥–∫—ñ–≤ –¥–æ –æ—á–∏—â–µ–Ω–Ω—è: {cleaning_stats['total_rows_before']}")
    print(f"   - –†—è–¥–∫—ñ–≤ –ø—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è: {cleaning_stats['total_rows_after']}")
    print(f"   - –í–∏–¥–∞–ª–µ–Ω–æ —Ä—è–¥–∫—ñ–≤: {cleaning_stats['total_rows_before'] - cleaning_stats['total_rows_after']}")
    print(f"   - –ó–º–µ–Ω—à–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {cleaned_index_data['cleaning_stats']['data_reduction_percent']}%")

    return True

if __name__ == "__main__":
    print("üßπ –ü–æ—á–∞—Ç–æ–∫ –æ—á–∏—â–µ–Ω–Ω—è Excel –¥–∞–Ω–∏—Ö...")
    clean_excel_data()
